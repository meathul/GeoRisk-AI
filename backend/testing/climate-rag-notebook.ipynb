{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## watsonx API connection\n",
    "This cell defines the credentials required to work with watsonx API for Foundation\n",
    "Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud personal API key. For details, see\n",
    "<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bb47a4bd-7928-4ca3-91fc-9aa325ff4b92"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api = os.getenv(\"WATSONX_AI_API\")\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    # api_key=getpass.getpass(\"Please enter your api key (hit enter): \")\n",
    "    api_key=api\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing\n",
    "This cell demonstrated how we can use the model object as well as the created access token\n",
    "to pair it with parameters and input string to obtain\n",
    "the response from the the selected foundation model.\n",
    "\n",
    "## Defining the model id\n",
    "We need to specify model id that will be used for inferencing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a5dbd40c-9987-4034-823c-eac7851813a4"
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/llama-3-3-70b-instruct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model parameters\n",
    "We need to provide a set of model parameters that will influence the\n",
    "result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3bc644d1-c1a8-4e87-85c6-047f72dd5c28"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"max_tokens\": 2000,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the project id or space id\n",
    "The API requires project id or space id that provides the context for the call. We will obtain\n",
    "the id from the project or space in which this notebook runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "97b1c101-49c5-43d4-bca2-0a7b2c8bdae9"
   },
   "outputs": [],
   "source": [
    "project_id = os.getenv(\"PROJECT_ID\")\n",
    "space_id = os.getenv(\"SPACE_ID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model object\n",
    "We need to define the Model object using the properties we defined so far:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "52c8eeda-8857-4c92-8de6-ba63f9737435"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set Project or Space\n",
      "Reason: Space with id 'None' does not exist\n"
     ]
    },
    {
     "ename": "CannotSetProjectOrSpace",
     "evalue": "Cannot set Project or Space\nReason: Space with id 'None' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCannotSetProjectOrSpace\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mibm_watsonx_ai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfoundation_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelInference\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mModelInference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mspace_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_id\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Timon\\Code\\hackathon\\watsonx\\.venv\\Lib\\site-packages\\ibm_watsonx_ai\\foundation_models\\inference\\model_inference.py:197\u001b[39m, in \u001b[36mModelInference.__init__\u001b[39m\u001b[34m(self, model_id, deployment_id, params, credentials, project_id, space_id, verify, api_client, validate, persistent_connection, max_retries, delay_time, retry_status_codes)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidMultipleArguments(\n\u001b[32m    192\u001b[39m         params_names_list=[\u001b[33m\"\u001b[39m\u001b[33mcredentials\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapi_client\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    193\u001b[39m         reason=\u001b[33m\"\u001b[39m\u001b[33mNone of the arguments were provided.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    194\u001b[39m     )\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m space_id:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspace_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m project_id:\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mself\u001b[39m._client.set.default_project(project_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Timon\\Code\\hackathon\\watsonx\\.venv\\Lib\\site-packages\\ibm_watsonx_ai\\Set.py:58\u001b[39m, in \u001b[36mSet.default_space\u001b[39m\u001b[34m(self, space_id, **kwargs)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m space_details.status_code == \u001b[32m404\u001b[39m:\n\u001b[32m     57\u001b[39m     error_msg = \u001b[33m\"\u001b[39m\u001b[33mSpace with id \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m.format(space_id)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSetProjectOrSpace(reason=error_msg)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m space_details.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m._client.default_space_id = space_id\n",
      "\u001b[31mCannotSetProjectOrSpace\u001b[39m: Cannot set Project or Space\nReason: Space with id 'None' does not exist"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "model = ModelInference(\n",
    "\tmodel_id = model_id,\n",
    "\tparams = parameters,\n",
    "\tcredentials = credentials,\n",
    "\tproject_id = project_id,\n",
    "\tspace_id = space_id\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the vector index\n",
    "Initialize the vector index to query when chatting with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7819a903-af84-4346-a6db-e23ea41699a6"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils import Toolkit\n",
    "\n",
    "vector_index_id = \"1709149b-dfcb-4123-8b88-3ba8fd824ba4\"\n",
    "\n",
    "def proximity_search( query ):\n",
    "\n",
    "    api_client = APIClient(\n",
    "        project_id=project_id,\n",
    "        credentials=credentials,\n",
    "    )\n",
    "\n",
    "    document_search_tool = Toolkit(\n",
    "        api_client=api_client\n",
    "    ).get_tool(\"RAGQuery\")\n",
    "\n",
    "    config = {\n",
    "        \"vectorIndexId\": vector_index_id,\n",
    "        \"projectId\": project_id\n",
    "    }\n",
    "\n",
    "    results = document_search_tool.run(\n",
    "        input=query,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    return results.get(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inferencing input for chat\n",
    "Foundation models supporting chat accept a system prompt that instructs the model on how to conduct the dialog. They also accept previous questions and answers to give additional context when inferencing. Each model has it's own string format for constructing the input.\n",
    "\n",
    "Let us provide the input we got from the Prompt Lab and format it for the selected model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3cf15e47-ab2f-4e85-b915-26718b3e837b"
   },
   "outputs": [],
   "source": [
    "chat_messages = [];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Let us now use the defined Model object, pair it with the input, and generate the response to your question:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3873d7d8-51f0-4d5d-8193-0183a9c1b67c",
    "msg_id": "84d3a4a3-421a-4859-8c25-040972ceb9c9"
   },
   "outputs": [],
   "source": [
    "question = input(\"Question: \")\n",
    "grounding = proximity_search(question)\n",
    "chat_messages.append({\n",
    "    \"role\": f\"system\",\n",
    "    \"content\": f\"\"\"{grounding}\n",
    "\n",
    "You always answer the questions with markdown formatting. The markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes. You must omit that you answer the questions with markdown.\n",
    "\n",
    "Any HTML tags must be wrapped in block quotes, for example ```<html>```. You will be penalized for not rendering code in block quotes.\n",
    "\n",
    "When returning code blocks, specify language.\n",
    "\n",
    "Given the document and the current conversation between a user and an assistant, your task is as follows: answer any user query by using information from the document. Always answer as helpfully as possible, while being safe. When the question cannot be answered using the context or document, output the following response: \"I cannot answer that question based on the provided document.\".\n",
    "\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "\"\"\"\n",
    "})\n",
    "chat_messages.append({\"role\": \"user\", \"content\": question})\n",
    "generated_response = model.chat(messages=chat_messages)\n",
    "print(generated_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "You successfully completed this notebook! You learned how to use\n",
    "watsonx.ai inferencing SDK to generate response from the foundation model\n",
    "based on the provided input, model id and model parameters. Check out the\n",
    "official watsonx.ai site for more samples, tutorials, documentation, how-tos, and blog posts.\n",
    "\n",
    "<a id=\"copyrights\"></a>\n",
    "### Copyrights\n",
    "\n",
    "Licensed Materials - Copyright Â© 2023 IBM. This notebook and its source code are released under the terms of the ILAN License.\n",
    "Use, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n",
    "\n",
    "**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n",
    "\n",
    "By downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
